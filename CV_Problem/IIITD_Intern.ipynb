{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IIITD_Intern.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MVu9Qk_g05CT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D37LW-uY1pf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_image.pkl', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "with open('train_label.pkl', 'rb') as f:\n",
        "    train_labels = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xClLJjWJ2mX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "881e1c96-2f30-4226-957e-713f42975e3e"
      },
      "cell_type": "code",
      "source": [
        "train_images = np.array(train_images).reshape((8000, 28, 28, 1)).astype('uint8')\n",
        "img = Image.fromarray(train_images[5000].reshape((28, 28)).astype('uint8'), 'L')\n",
        "plt.imshow(img)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f24c693f6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF/9JREFUeJzt3XtM1ff9x/HXKRc5VAyVW2ev6tAR\nrUmX2IhMV9DZ0MS09o+xEnXNusRu02iN64wpttGkVnQmOpehtrqkZMnZ+GeadMM4s8UQwMkfTSAW\nUDfnFblVoRwQkN8f/fWEy+Hw/h7O4Zyjz8dfns/33c/5fD305fecL+/zcQ0NDQ0JABDQY5FeAADE\nAsISAAwISwAwICwBwICwBAADwhIADOIjvQCEX0NDg7l2+/btfscPHz6sjRs3+h6/+OKL5jl/+tOf\nmmvnzp1rrrXyer1+x6dNm6a+vr4RYzdv3jTNeeDAAfPzt7e3m2vLysrMtampqeZaTB5XljB57rnn\nIr2EkHvsMX78YRf0leVHH32kL774Qi6XSzt27NCiRYtCuS4AiCpBheX58+d19epVeTweXb58WTt2\n7JDH4wn12gAgagT1PqS6ulorV66U9M1nTHfv3lV3d3dIFwYA0SSoK8u2tjYtWLDA93jmzJlqbW3V\n9OnTQ7YwhM7w12oip06dCupYNHO73eZj1htMv//97ye1JsSekNwN57s4olso7oafOnVKq1ev9j1+\nGO6Gu93uMce4G47xBPU2PDMzU21tbb7Hd+7cUUZGRsgWBQDRJqiwzMvLU2VlpaRvrloyMzN5Cw7g\noRbU2/Dvf//7WrBggX7yk5/I5XLpgw8+CPW6ACCqBP2Z5bZt20K5DgCIai6+KT02Xb582VxbUFBg\nrn366af9jldVVSkvL8/3uKOjwzznl19+aa5NSEgw1fX395vnHM/Q0JBcLteIsZycHNN/GxcXZ36e\n+Hj7Ncn9+/fNtVVVVWPGUlNT9dVXX40Zw+TR7wUABoQlABgQlgBgQFgCgAFhCQAGhCUAGBCWAGBA\nWAKAAWEJAAZsWBajfvvb35prnXSbBKodfszJF6f86Ec/MteO3kBsPL29veY5k5KSxj22fPly8zzD\n9fT0mGunTZtmrm1paTHX+vuauF27do0Z37Vrl3lOjI8rSwAwICwBwICwBAADwhIADAhLADAgLAHA\ngLAEAAPCEgAMCEsAMCAsAcCAdscY1dbWFpZ5vV5vUMcCaW1tNddaWwOdtFsGak0cvUGYdcM0t9tt\nfv7RG4gFMnoDtUCampocjWNyuLIEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAAD\nwhIADGh3jFF1dXXm2qGhIXNtZ2en6Vh6erp5zqysLHOttTXQyY6VgVoTRx8LtBPkcDdv3jQ/v5N2\nRyevVXd3t6NxTA5XlgBgQFgCgAFhCQAGhCUAGBCWAGBAWAKAAWEJAAaEJQAYEJYAYEAHT4yaMWOG\nufbevXvm2kDdH8OPOdmwq6ury1ybmJhoquvv7zfPGej8b926NeKxtYPHySZsTjqYOjo6zLWXL192\nNI7J4coSAAyCurKsra3V5s2blZ2dLUmaN2+eSkpKQrowAIgmQb8Nf+mll3To0KFQrgUAohZvwwHA\nIOiwvHTpkt555x29+eabqqqqCuWaACDquIacfIHe/2tpaVFdXZ0KCwt17do1rV+/XqdPnzbfyQSA\nWBPUZ5ZZWVl69dVXJUnPPvus0tPT1dLSomeeeSaki8P4XnzxRXOtk18d+vrrr/2O3759W08++aTv\ncUZGhnnOBw8emGut/+A6+Yd5vPO/ePGicnJyRoxZf3Xozp075ucP168O+fv1LX/ndPHiRfOcGF9Q\nb8NPnjypTz/9VNI3v2/W3t7u6AcCAGJNUFeWBQUF2rZtm/7xj3+ov79fH374IW/BATzUggrL6dOn\nq6ysLNRrAYCoRbtjlBnvM8PR7t69a57TyT28np4e07G+vj7znE4MDAyY6gKtc7RA5z84ODjicW9v\nr2nO+Hj7/zo3btww1zox3nmN/rtx0m6akpIyqTU9zPg9SwAwICwBwICwBAADwhIADAhLADAgLAHA\ngLAEAAPCEgAMCEsAMCAsAcCAdscoc+nSJVOdtS1PktLS0sy1TzzxxLjHvvvd7/r+7KSFLzMz01xr\nbXd00m4YaM64uLgRj6dNm2aa83//+5/5+X/84x+ba2tqasy1nZ2dfsdHt6KO3sEyENodx8eVJQAY\nEJYAYEBYAoABYQkABoQlABgQlgBgQFgCgAFhCQAGhCUAGNDBE2WsnTFONiz7wQ9+YK595ZVXxj32\nq1/9yvfnn//85+Y5n3nmGXNtd3e3qS4pKck8p9frNR+zzutkw7Rdu3aZa3/961+ba//617/6HR/9\ns3H16lXznPPmzTPXPmq4sgQAA8ISAAwISwAwICwBwICwBAADwhIADAhLADAgLAHAgLAEAAPCEgAM\naHeMMq2traa60ZtSBZKdnW2uXbVqVVDHArFuQiZJbrfbVDc0NGSe8/HHHzcfS0xMNM9r5aTdc+HC\nhebaP//5z37HR/9sdHR0mOfE+LiyBAADwhIADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQA\nA8ISAAxod4wyNTU1prrBwUHznK+//rq5NlBropO2xeEWLFhgrr148aKpLtCOjaOlpqaaj82ZM8c0\n5/nz583P78TSpUvNteO1fI4ev3DhgnnOoqIic+2jxnRl2dTUpJUrV6q8vFySdOvWLa1bt07FxcXa\nvHmz7t+/H9ZFAkCkTRiWPT092r17t3Jzc31jhw4dUnFxsf70pz/pueeeU0VFRVgXCQCRNmFYJiYm\n6tixY8rMzPSN1dbWasWKFZKk/Px8VVdXh2+FABAFJvzMMj4+XvHxI8u8Xq/vq6zS0tLMXysGALFq\n0jd4nHyvICb2hz/8IaR1oTR79mzfnx+W172qqiqo/+6zzz4L8Uq+UVBQYK613uBBaAQVlsnJyert\n7VVSUpJaWlpGvEXH5PziF78w1ZWVlZnndHLnNj093e/47Nmz9Z///Mf32HrXWJKKi4vNteG4Gz5z\n5ky/41VVVcrLyxsxZj2vb292WjgJr7Nnz5prv/0obPRzuVyuEWPbtm0zz7lv3z5z7aMmqN+zXLp0\nqSorKyVJp0+f1rJly0K6KACINhNeWdbX12vv3r26ceOG4uPjVVlZqf3792v79u3yeDyaNWuWo9/j\nA4BYNGFYLly40O/nMydOnAjLggAgGtHBE2XCsbnUk08+aa794x//6He8pKRkxOd0Tjb2unLlirnW\n+vlef3+/ec6WlhbzMScbwVn95S9/MdeO/gw1FDo7O0M+56OI3nAAMCAsAcCAsAQAA8ISAAwISwAw\nICwBwICwBAADwhIADAhLADAgLAHAgHbHKHPnzp2Qz9nc3Gyu/fvf/+53vKSkZMQxJ1/R5qQ1saur\ny1SXkJBgntPJPNb9pObNm2d+joMHD5prU1JSzLVW169fD/mcjyKuLAHAgLAEAAPCEgAMCEsAMCAs\nAcCAsAQAA8ISAAwISwAwICwBwICwBAAD2h2jzKxZs0x1xcXF5jnnzp1rrq2pqTEdmz9/vnlOJ7sL\nPnjwwFxrNTAwMO4xr9c74vHg4KBpzri4OPPzV1VVmWsXLVpkrv3Zz35mGne73eY5MT6uLAHAgLAE\nAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwMA1NDQ0FOlFILzu3btnrh1vI7K2tjalp6f7\nHn/nO98xzzm6SyYQJ50xVuP9iDc1NY3ZeCxQt89wTrpi2trazLXXrl0z1yYmJpprMXlcWQKAAWEJ\nAAaEJQAYEJYAYEBYAoABYQkABoQlABgQlgBgQFgCgAFhCQAGbFj2CPjXv/5lrg3U/Tr8mMvlMs/p\npIWxv7/fVPfYY/Z/5wM9/+jzsG5Y5qRL2NpCKUn//ve/zbV5eXnmWkweV5YAYGAKy6amJq1cuVLl\n5eWSpO3bt2v16tVat26d1q1bp3/+85/hXCMARNyEb8N7enq0e/du5ebmjhjfunWr8vPzw7YwAIgm\nE15ZJiYm6tixY8rMzJyK9QBAVJrwyjI+Pl7x8WPLysvLdeLECaWlpamkpEQzZ84MywIxeatXrzbX\ntre3B3UsVjU2NkZ6CYgRQd0Nf+2115SamqqcnBwdPXpUhw8f1s6dO0O9NoTIqVOnzLVvvfWW3/H2\n9nalpaX5Hj/11FPmOfv6+sy1U3k3vLGxUfPnzx8x1tvba5rz8ccfNz9/S0uLufbkyZPmWu6GT62g\n7obn5uYqJydHklRQUKCmpqaQLgoAok1QYblp0ybf19/X1tYqOzs7pIsCgGgz4dvw+vp67d27Vzdu\n3FB8fLwqKyu1du1abdmyRW63W8nJydqzZ89UrBUAImbCsFy4cKE+++yzMeOvvPJKWBYEANGIdsco\nY22jc9JuWF9fb64N1O5nbQUczXrTJFyctFuG4+///v375tq6ujpzrfUGj5PWTCfn9aih3READAhL\nADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwoN0xyoSj3ezy5cvm2kCtgcOPOWl9\ndPLdk+GY07pjpWTfidHJjo0JCQnm2traWnOtFS2MocGVJQAYEJYAYEBYAoABYQkABoQlABgQlgBg\nQFgCgAFhCQAGhCUAGNDB8wgI1YZVw489ePDAPKeTDpaenh5TnZMOokDPP3oea7eLk03InJx/YmKi\nuRZTiytLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwIB2x0fAnTt3zLXx\n8eP/SAw/5qTd0ElroHVeJy2E1k3YnDy/k3ZPJ5ur3b5921yLqcWVJQAYEJYAYEBYAoABYQkABoQl\nABgQlgBgQFgCgAFhCQAGhCUAGBCWAGBAu+MjoKOjw1xrbXd0wrpjomRvDQzHjpWSvd2xv7/f/PxO\ndmxsbW0112JqmX76S0tLVVdXp4GBAW3YsEEvvPCC3nvvPQ0ODiojI0P79u1jC08AD7UJw7KmpkbN\nzc3yeDzq7OzUmjVrlJubq+LiYhUWFurAgQOqqKhQcXHxVKwXACJiwvc8ixcv1sGDByVJM2bMkNfr\nVW1trVasWCFJys/PV3V1dXhXCQARNmFYxsXFKTk5WZJUUVGh5cuXy+v1+t52p6Wl8TkLgIee+RP7\nM2fOqKKiQsePH9eqVat8404+aEdkVFVVhWSe69evh2SeaNLY2BjpJSBGmMLy3LlzKisr0yeffKKU\nlBQlJyert7dXSUlJamlpUWZmZrjXiUnIy8sz1169etXv+PXr1/X000/7Hn/7bsPCyZ1j6xcFO/ny\n32nTpvkdb2xs1Pz580eMWX9zwMnzO6nNyMgw1164cMFci8mb8G14V1eXSktLdeTIEaWmpkqSli5d\nqsrKSknS6dOntWzZsvCuEgAibMIry88//1ydnZ3asmWLb+zjjz/W+++/L4/Ho1mzZun1118P6yIB\nINImDMuioiIVFRWNGT9x4kRYFgQA0YgOnkdAW1ubudbawTMwMGCe08mGXU42QrMa7zNLfwJtbjac\nk3U6adhob28312Jq0RsOAAaEJQAYEJYAYEBYAoABYQkABoQlABgQlgBgQFgCgAFhCQAGhCUAGNDu\nGKN6e3vNtd3d3ebapKSkcY8NbwV00u7n5DtPrV/RFmidowX6ijgnXx83nJPzd9Lu+dVXX5lrvV7v\nmDG32z1m3O12m+fE+LiyBAADwhIADAhLADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAxo\nd4xR9+7dM9f29fWZawPthDi8bdFJC9+DBw/MtU52jbRyuVxBHQvEyTkF2jFzNCfn76/l1e12jxmn\n3TE0uLIEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwICwBwICwBAADOnhi1N27d821TrpCEhIS\nTMesG4s5Zd3czMlGY046WKzdNk66opx0+4Si2ykcXVDgyhIATAhLADAgLAHAgLAEAAPCEgAMCEsA\nMCAsAcCAsAQAA8ISAAwISwAwoN0xRjlpNxwcHAz58zvZ6Mvawuhk3kAbqzl5/tHHrBuxOTknJ5y0\nO3Z1dY0Zy8jIGDOekZEx6XXBGJalpaWqq6vTwMCANmzYoLNnz6qhoUGpqamSpLffflsvv/xyONcJ\nABE1YVjW1NSoublZHo9HnZ2dWrNmjZYsWaKtW7cqPz9/KtYIABE3YVguXrxYixYtkiTNmDFDXq83\nLG/rACCaTfgBTVxcnJKTkyVJFRUVWr58ueLi4lReXq7169fr3XffVUdHR9gXCgCR5BoyflJ95swZ\nHTlyRMePH1d9fb1SU1OVk5Ojo0eP6vbt29q5c2e41woAEWO6wXPu3DmVlZXpk08+UUpKinJzc33H\nCgoK9OGHH4ZrfRhHQ0ODuXbJkiXm2lmzZvkdb2xs1Pz5832PnXz5rpM7vNZ3KTNmzDDPmZiY6Hf8\nypUrmjNnzogx6xfn3rt3z/z86enp5tqbN2+aa+vr68eMzZkzR1euXBkzhsmb8G14V1eXSktLdeTI\nEd/d702bNunatWuSpNraWmVnZ4d3lQAQYRNeWX7++efq7OzUli1bfGNvvPGGtmzZIrfbreTkZO3Z\nsyesiwSASJswLIuKilRUVDRmfM2aNWFZEABEI9odAcCAdscYNfpD/EC+/vprc63X6zUds+6CONGc\no/lr4fPHyfMH2rFy9C+D9PT0mOZ08nealJRkrnXyd+XvZ4AbPOHDlSUAGBCWAGBAWAKAAWEJAAaE\nJQAYEJYAYEBYAoABYQkABoQlABiYv88Ssevs2bPm2n379vkd/9vf/qbCwkLf4y+//NI8Z0pKirnW\n+nVmcXFx5jmbm5v9jv/3v//V888/P2LMurmX9avcJOmpp54y1/7mN78x1y5btsxci8njyhIADAhL\nADAgLAHAgLAEAAPCEgAMCEsAMCAsAcCAsAQAA8ISAAwISwAwoN0RAAy4sgQAA8ISAAwISwAwICwB\nwICwBAADwhIADOIj8aQfffSRvvjiC7lcLu3YsUOLFi2KxDJCqra2Vps3b1Z2drYkad68eSopKYnw\nqoLX1NSkX/7yl3rrrbe0du1a3bp1S++9954GBweVkZGhffv2KTExMdLLdGT0OW3fvl0NDQ1KTU2V\nJL399tt6+eWXI7tIh0pLS1VXV6eBgQFt2LBBL7zwQsy/TtLY8zp79mzEX6spD8vz58/r6tWr8ng8\nunz5snbs2CGPxzPVywiLl156SYcOHYr0Miatp6dHu3fvVm5urm/s0KFDKi4uVmFhoQ4cOKCKigoV\nFxdHcJXO+DsnSdq6davy8/MjtKrJqampUXNzszwejzo7O7VmzRrl5ubG9Osk+T+vJUuWRPy1mvK3\n4dXV1Vq5cqUkae7cubp79666u7unehkIIDExUceOHVNmZqZvrLa2VitWrJAk5efnq7q6OlLLC4q/\nc4p1ixcv1sGDByVJM2bMkNfrjfnXSfJ/XoODgxFeVQTCsq2tTU888YTv8cyZM9Xa2jrVywiLS5cu\n6Z133tGbb76pqqqqSC8naPHx8UpKShox5vV6fW/n0tLSYu4183dOklReXq7169fr3XffVUdHRwRW\nFry4uDglJydLkioqKrR8+fKYf50k/+cVFxcX8dcqIp9ZDvewdFs+//zz2rhxowoLC3Xt2jWtX79e\np0+fjsnPiybysLxmr732mlJTU5WTk6OjR4/q8OHD2rlzZ6SX5diZM2dUUVGh48ePa9WqVb7xWH+d\nhp9XfX19xF+rKb+yzMzMVFtbm+/xnTt3zNuPRrOsrCy9+uqrcrlcevbZZ5Wenq6WlpZILytkkpOT\n1dvbK0lqaWl5KN7O5ubmKicnR5JUUFCgpqamCK/IuXPnzqmsrEzHjh1TSkrKQ/M6jT6vaHitpjws\n8/LyVFlZKUlqaGhQZmampk+fPtXLCLmTJ0/q008/lSS1traqvb1dWVlZEV5V6CxdutT3up0+ffqh\n2LN606ZNunbtmqRvPpP99jcZYkVXV5dKS0t15MgR313ih+F18nde0fBaReRbh/bv368LFy7I5XLp\ngw8+0Pe+972pXkLIdXd3a9u2bbp37576+/u1ceNG/fCHP4z0soJSX1+vvXv36saNG4qPj1dWVpb2\n79+v7du3q6+vT7NmzdKePXuUkJAQ6aWa+TuntWvX6ujRo3K73UpOTtaePXuUlpYW6aWaeTwe/e53\nv9Ps2bN9Yx9//LHef//9mH2dJP/n9cYbb6i8vDyirxVf0QYABnTwAIABYQkABoQlABgQlgBgQFgC\ngAFhCQAGhCUAGBCWAGDwfzDfPfdWJL6CAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Qw-unpXiiJM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87ee160a-6efb-4ef7-c3ed-4af1127683fe"
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(train_labels)):\n",
        "  if train_labels[i] == 2:\n",
        "    train_labels[i] = 1\n",
        "  elif train_labels[i] == 3:\n",
        "    train_labels[i] = 2\n",
        "  elif train_labels[i] == 6:\n",
        "    train_labels[i] = 3\n",
        "    \n",
        "print(set(train_labels))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zkHs8OpR2yrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "27a9a4f4-721d-437a-bf09-88fa84a6b543"
      },
      "cell_type": "code",
      "source": [
        "train_images, train_labels = shuffle(train_images, train_labels)\n",
        "img = Image.fromarray(train_images[5000].reshape((28, 28)).astype('uint8'), 'L')\n",
        "print(train_labels[5000])\n",
        "plt.imshow(img)\n",
        "\n",
        "train_images = train_images.astype('float32')/255.0\n",
        "train_labels = np_utils.to_categorical(train_labels, 4)\n",
        "print(train_labels.shape)\n",
        "print(train_labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(8000, 4)\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " ...\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABj8HzeZb2rQAWLAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YSPmyGVNf_Od",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "964b7185-a23e-4fad-e982-544fcd4b0753"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "inputShape = (28, 28, 1)\n",
        "\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(4))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_13 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4)                 1028      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 1,806,308\n",
            "Trainable params: 1,804,900\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FmLo3l3ataf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"model-{val_loss:.2f}.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=True, mode=\"min\", period=1)\n",
        "\n",
        "stop = EarlyStopping(monitor=\"val_loss\", patience=50, mode=\"min\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0-SY0TWgawC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4678
        },
        "outputId": "ceb325c5-732f-4d0f-e51e-a38afa187777"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "history = model.fit(train_images, train_labels, validation_split = 0.2, shuffle = True, batch_size = 256, epochs=200, callbacks=[checkpoint,  stop])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6400 samples, validate on 1600 samples\n",
            "Epoch 1/200\n",
            "6400/6400 [==============================] - 4s 633us/step - loss: 1.2173 - acc: 0.6573 - val_loss: 0.6547 - val_acc: 0.7706\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.65473, saving model to model-0.65.h5\n",
            "Epoch 2/200\n",
            "6400/6400 [==============================] - 2s 238us/step - loss: 0.7727 - acc: 0.7445 - val_loss: 0.4736 - val_acc: 0.8275\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.65473 to 0.47358, saving model to model-0.47.h5\n",
            "Epoch 3/200\n",
            "6400/6400 [==============================] - 2s 239us/step - loss: 0.6335 - acc: 0.7783 - val_loss: 0.4523 - val_acc: 0.8381\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.47358 to 0.45230, saving model to model-0.45.h5\n",
            "Epoch 4/200\n",
            "6400/6400 [==============================] - 2s 238us/step - loss: 0.5639 - acc: 0.8028 - val_loss: 0.4099 - val_acc: 0.8475\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.45230 to 0.40992, saving model to model-0.41.h5\n",
            "Epoch 5/200\n",
            "6400/6400 [==============================] - 2s 238us/step - loss: 0.5057 - acc: 0.8105 - val_loss: 0.4005 - val_acc: 0.8419\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.40992 to 0.40053, saving model to model-0.40.h5\n",
            "Epoch 6/200\n",
            "6400/6400 [==============================] - 2s 237us/step - loss: 0.5077 - acc: 0.8103 - val_loss: 0.3899 - val_acc: 0.8431\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.40053 to 0.38986, saving model to model-0.39.h5\n",
            "Epoch 7/200\n",
            "6400/6400 [==============================] - 2s 238us/step - loss: 0.4537 - acc: 0.8248 - val_loss: 0.4571 - val_acc: 0.8337\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.38986\n",
            "Epoch 8/200\n",
            "6400/6400 [==============================] - 2s 237us/step - loss: 0.4183 - acc: 0.8456 - val_loss: 0.3877 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.38986 to 0.38774, saving model to model-0.39.h5\n",
            "Epoch 9/200\n",
            "6400/6400 [==============================] - 2s 240us/step - loss: 0.4056 - acc: 0.8478 - val_loss: 0.4051 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.38774\n",
            "Epoch 10/200\n",
            "6400/6400 [==============================] - 2s 238us/step - loss: 0.3839 - acc: 0.8548 - val_loss: 0.3472 - val_acc: 0.8638\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.38774 to 0.34716, saving model to model-0.35.h5\n",
            "Epoch 11/200\n",
            "6400/6400 [==============================] - 2s 239us/step - loss: 0.3735 - acc: 0.8548 - val_loss: 0.3736 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.34716\n",
            "Epoch 12/200\n",
            "6400/6400 [==============================] - 2s 235us/step - loss: 0.3632 - acc: 0.8630 - val_loss: 0.3248 - val_acc: 0.8725\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.34716 to 0.32477, saving model to model-0.32.h5\n",
            "Epoch 13/200\n",
            "6400/6400 [==============================] - 2s 239us/step - loss: 0.3529 - acc: 0.8661 - val_loss: 0.3449 - val_acc: 0.8612\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.32477\n",
            "Epoch 14/200\n",
            "6400/6400 [==============================] - 2s 235us/step - loss: 0.3317 - acc: 0.8711 - val_loss: 0.3403 - val_acc: 0.8706\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.32477\n",
            "Epoch 15/200\n",
            "6400/6400 [==============================] - 2s 239us/step - loss: 0.3183 - acc: 0.8758 - val_loss: 0.3301 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.32477\n",
            "Epoch 16/200\n",
            "6400/6400 [==============================] - 2s 240us/step - loss: 0.3154 - acc: 0.8762 - val_loss: 0.3390 - val_acc: 0.8712\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.32477\n",
            "Epoch 17/200\n",
            "6400/6400 [==============================] - 2s 246us/step - loss: 0.3066 - acc: 0.8809 - val_loss: 0.3780 - val_acc: 0.8525\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.32477\n",
            "Epoch 18/200\n",
            "6400/6400 [==============================] - 2s 246us/step - loss: 0.2879 - acc: 0.8855 - val_loss: 0.3063 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.32477 to 0.30627, saving model to model-0.31.h5\n",
            "Epoch 19/200\n",
            "6400/6400 [==============================] - 2s 240us/step - loss: 0.2794 - acc: 0.8914 - val_loss: 0.3225 - val_acc: 0.8762\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.30627\n",
            "Epoch 20/200\n",
            "6400/6400 [==============================] - 2s 240us/step - loss: 0.2673 - acc: 0.8958 - val_loss: 0.3355 - val_acc: 0.8700\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.30627\n",
            "Epoch 21/200\n",
            "6400/6400 [==============================] - 2s 240us/step - loss: 0.2669 - acc: 0.8953 - val_loss: 0.3268 - val_acc: 0.8850\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.30627\n",
            "Epoch 22/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.2609 - acc: 0.8952 - val_loss: 0.3256 - val_acc: 0.8825\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.30627\n",
            "Epoch 23/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.2571 - acc: 0.9005 - val_loss: 0.3120 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.30627\n",
            "Epoch 24/200\n",
            "6400/6400 [==============================] - 2s 255us/step - loss: 0.2475 - acc: 0.9028 - val_loss: 0.3724 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.30627\n",
            "Epoch 25/200\n",
            "6400/6400 [==============================] - 2s 246us/step - loss: 0.2426 - acc: 0.9066 - val_loss: 0.3219 - val_acc: 0.8769\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.30627\n",
            "Epoch 26/200\n",
            "6400/6400 [==============================] - 2s 252us/step - loss: 0.2291 - acc: 0.9086 - val_loss: 0.3650 - val_acc: 0.8669\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.30627\n",
            "Epoch 27/200\n",
            "6400/6400 [==============================] - 2s 250us/step - loss: 0.2187 - acc: 0.9181 - val_loss: 0.3204 - val_acc: 0.8831\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.30627\n",
            "Epoch 28/200\n",
            "6400/6400 [==============================] - 2s 254us/step - loss: 0.2145 - acc: 0.9156 - val_loss: 0.3175 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.30627\n",
            "Epoch 29/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.2079 - acc: 0.9208 - val_loss: 0.3238 - val_acc: 0.8812\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.30627\n",
            "Epoch 30/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.2007 - acc: 0.9202 - val_loss: 0.3607 - val_acc: 0.8738\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.30627\n",
            "Epoch 31/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.2002 - acc: 0.9245 - val_loss: 0.3377 - val_acc: 0.8812\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.30627\n",
            "Epoch 32/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.2006 - acc: 0.9266 - val_loss: 0.3286 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.30627\n",
            "Epoch 33/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1934 - acc: 0.9245 - val_loss: 0.3198 - val_acc: 0.8894\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.30627\n",
            "Epoch 34/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1772 - acc: 0.9330 - val_loss: 0.3230 - val_acc: 0.8838\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.30627\n",
            "Epoch 35/200\n",
            "6400/6400 [==============================] - 2s 250us/step - loss: 0.1840 - acc: 0.9266 - val_loss: 0.4282 - val_acc: 0.8488\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.30627\n",
            "Epoch 36/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1740 - acc: 0.9342 - val_loss: 0.3229 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.30627\n",
            "Epoch 37/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.1710 - acc: 0.9341 - val_loss: 0.3485 - val_acc: 0.8812\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.30627\n",
            "Epoch 38/200\n",
            "6400/6400 [==============================] - 2s 247us/step - loss: 0.1661 - acc: 0.9342 - val_loss: 0.3568 - val_acc: 0.8800\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.30627\n",
            "Epoch 39/200\n",
            "6400/6400 [==============================] - 2s 253us/step - loss: 0.1709 - acc: 0.9342 - val_loss: 0.3332 - val_acc: 0.8794\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.30627\n",
            "Epoch 40/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.1524 - acc: 0.9464 - val_loss: 0.3260 - val_acc: 0.8881\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.30627\n",
            "Epoch 41/200\n",
            "6400/6400 [==============================] - 2s 252us/step - loss: 0.1483 - acc: 0.9462 - val_loss: 0.3482 - val_acc: 0.8806\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.30627\n",
            "Epoch 42/200\n",
            "6400/6400 [==============================] - 2s 244us/step - loss: 0.1526 - acc: 0.9413 - val_loss: 0.4208 - val_acc: 0.8706\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.30627\n",
            "Epoch 43/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1510 - acc: 0.9423 - val_loss: 0.3249 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.30627\n",
            "Epoch 44/200\n",
            "6400/6400 [==============================] - 2s 247us/step - loss: 0.1364 - acc: 0.9475 - val_loss: 0.3302 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.30627\n",
            "Epoch 45/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1443 - acc: 0.9453 - val_loss: 0.3544 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.30627\n",
            "Epoch 46/200\n",
            "6400/6400 [==============================] - 2s 253us/step - loss: 0.1426 - acc: 0.9466 - val_loss: 0.4181 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.30627\n",
            "Epoch 47/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1332 - acc: 0.9491 - val_loss: 0.3343 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.30627\n",
            "Epoch 48/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1295 - acc: 0.9503 - val_loss: 0.3905 - val_acc: 0.8806\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.30627\n",
            "Epoch 49/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1378 - acc: 0.9462 - val_loss: 0.3691 - val_acc: 0.8881\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.30627\n",
            "Epoch 50/200\n",
            "6400/6400 [==============================] - 2s 252us/step - loss: 0.1182 - acc: 0.9537 - val_loss: 0.3608 - val_acc: 0.8900\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.30627\n",
            "Epoch 51/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.1133 - acc: 0.9575 - val_loss: 0.3918 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.30627\n",
            "Epoch 52/200\n",
            "6400/6400 [==============================] - 2s 250us/step - loss: 0.1172 - acc: 0.9533 - val_loss: 0.3661 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.30627\n",
            "Epoch 53/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1201 - acc: 0.9550 - val_loss: 0.3485 - val_acc: 0.8931\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.30627\n",
            "Epoch 54/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.1122 - acc: 0.9563 - val_loss: 0.3782 - val_acc: 0.8831\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.30627\n",
            "Epoch 55/200\n",
            "6400/6400 [==============================] - 2s 246us/step - loss: 0.1152 - acc: 0.9530 - val_loss: 0.4210 - val_acc: 0.8781\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.30627\n",
            "Epoch 56/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1083 - acc: 0.9577 - val_loss: 0.3467 - val_acc: 0.8988\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.30627\n",
            "Epoch 57/200\n",
            "6400/6400 [==============================] - 2s 250us/step - loss: 0.1147 - acc: 0.9567 - val_loss: 0.3374 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.30627\n",
            "Epoch 58/200\n",
            "6400/6400 [==============================] - 2s 249us/step - loss: 0.1081 - acc: 0.9597 - val_loss: 0.3659 - val_acc: 0.8919\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.30627\n",
            "Epoch 59/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.0999 - acc: 0.9625 - val_loss: 0.3871 - val_acc: 0.8875\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.30627\n",
            "Epoch 60/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1051 - acc: 0.9569 - val_loss: 0.4050 - val_acc: 0.8869\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.30627\n",
            "Epoch 61/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.1032 - acc: 0.9634 - val_loss: 0.3556 - val_acc: 0.9062\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.30627\n",
            "Epoch 62/200\n",
            "6400/6400 [==============================] - 2s 247us/step - loss: 0.0913 - acc: 0.9664 - val_loss: 0.3692 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.30627\n",
            "Epoch 63/200\n",
            "6400/6400 [==============================] - 2s 251us/step - loss: 0.0859 - acc: 0.9672 - val_loss: 0.3939 - val_acc: 0.8938\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.30627\n",
            "Epoch 64/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.0891 - acc: 0.9686 - val_loss: 0.3931 - val_acc: 0.8925\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.30627\n",
            "Epoch 65/200\n",
            "6400/6400 [==============================] - 2s 250us/step - loss: 0.0855 - acc: 0.9675 - val_loss: 0.3805 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.30627\n",
            "Epoch 66/200\n",
            "6400/6400 [==============================] - 2s 248us/step - loss: 0.0746 - acc: 0.9695 - val_loss: 0.3861 - val_acc: 0.9006\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.30627\n",
            "Epoch 67/200\n",
            "6400/6400 [==============================] - 2s 239us/step - loss: 0.0860 - acc: 0.9691 - val_loss: 0.4239 - val_acc: 0.8838\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.30627\n",
            "Epoch 68/200\n",
            "6400/6400 [==============================] - 2s 242us/step - loss: 0.0912 - acc: 0.9669 - val_loss: 0.3924 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.30627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cKLUaTfpjGLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}