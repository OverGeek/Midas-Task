{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Problem.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "X58KY4V3C3bC",
        "qMmT3TjvDDJS",
        "WFc8vlw1b-uC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O5AQm-hWstno",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "I have applied 3 approaches to solve the image classification problem, out of which the second approach of using simple CNNs worked out to be best. I had also used data augmentation using ImageDataGenerator function of keras but the results were almost the same as the ones got without using data augmentation. Since using data augmentation has quite a big impact on training time, i did not find the results motivating enough to use data augmentation and hence i have not used data augmentation in this notebook.\n",
        "\n",
        "There in total 4 sections including this introduction, the other 3 are : \n",
        "\n",
        "\n",
        "*   Approach-1 (Using ANNs)\n",
        "*   Approach-1 (Using CNNs)\n",
        "*   Approach-1 (Using pretrained CNNs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "X58KY4V3C3bC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Approach-1( Using ANNs) "
      ]
    },
    {
      "metadata": {
        "id": "zb18g7O-IEdO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing Required Libraries**"
      ]
    },
    {
      "metadata": {
        "id": "goGiLUFoDIG0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, Activation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xlo6aYsLIKP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reading FIles**\n",
        "\n",
        "Using Python's pickle library to read pickle files.\n",
        "\n",
        "*   Training image data is present in train_images\n",
        "*   Training Labels are present in train_labels\n",
        "*   Test image data is present in test_images\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sqKgsc75DbFt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_image.pkl', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "with open('train_label.pkl', 'rb') as f:\n",
        "    train_labels = pickle.load(f)\n",
        "    \n",
        "with open('test_image.pkl', 'rb') as f:\n",
        "    test_images = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2hyrUvjuIteS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Going Through the data and preprocessing**\n",
        "\n",
        "First converted all the read files into numpy arrays, then checked their shape for understanding the structure of data"
      ]
    },
    {
      "metadata": {
        "id": "8H5UYodrEFrj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C70yXNnuLLnB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Feature Scaling**\n",
        "\n",
        "Using sklearn's standardScaler function to transform data such that it's distribution will have a mean value 0 and standard deviation of 1. This helps in better fitting the model in less time."
      ]
    },
    {
      "metadata": {
        "id": "GXQBbBLMEoS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "train_images = sc.fit_transform(train_images)\n",
        "test_images = sc.transform(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wmbhw3uIL6bZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**\n",
        "\n",
        "Since the problem is multi-label classification problem, we first convert the labels in on hot encoded form using to_categorical function from keras library. The function returns one hot encoded vector with number of labels = maximum value of label present in the passed train_labels. SInce the labels present in the train_labels are {0, 2, 3, 6}, we mapped the labels as follows:\n",
        "\n",
        "0 -> 0\n",
        "2 -> 1\n",
        "3 -> 2\n",
        "6 -> 3\n",
        "\n",
        "Before final submission, we will transform the results back to original labels"
      ]
    },
    {
      "metadata": {
        "id": "MQIpnYqZFiKl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(set(train_labels))\n",
        "    \n",
        "for i in range(len(train_labels)):\n",
        "  if train_labels[i] == 2:\n",
        "    train_labels[i] = 1\n",
        "  elif train_labels[i] == 3:\n",
        "    train_labels[i] = 2\n",
        "  elif train_labels[i] == 6:\n",
        "    train_labels[i] = 3\n",
        "    \n",
        "print(set(train_labels))\n",
        "\n",
        "train_labels = np_utils.to_categorical(np.array(train_labels))\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4OyfdGjNsDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Building the model**\n",
        "\n",
        "Here we build our ANN model with 8 layers using Dense and Dropouts layers. All the layers except the last have relu activation to introduce non-linearity. The last layer uses softmax activation to output probabilities of each class."
      ]
    },
    {
      "metadata": {
        "id": "jyNT0DNFF5tf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape = (784,), activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(256, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(128, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(64, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(32, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(16, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(8, activation = 'relu', kernel_initializer = 'uniform'))\n",
        "model.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "idZtGIhXOiSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Some Tuning before finally fitting the model**\n",
        "\n",
        "To save our model weights, we used ModelCheckpoint, it's parameters are set in order to save the weights to the current working directory every time validation accuracy improves.\n",
        "\n",
        "We used Early Stopping to stop training the model once the validation accuracy stops increasing after a set number of epochs (patience value).\n",
        "\n",
        "We used ReduceLROnPlateau to reduce the learning rate by a set factor each time validation accuracy stops increasing after a set number of epochs (patience value)."
      ]
    },
    {
      "metadata": {
        "id": "PiwDqFjWHTVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"model_ANN-{val_acc:.2f}.h5\", monitor=\"val_acc\", verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=True, mode=\"max\", period=1)\n",
        "\n",
        "stop = EarlyStopping(monitor=\"val_acc\", patience=50, mode=\"max\")\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
        "                                            patience = 3,\n",
        "                                            #patience=5, \n",
        "                                            verbose=1, \n",
        "                                            factor = 0.5,\n",
        "                                            #factor=0.25, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tc32AMr1QHAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training The model**\n",
        "\n",
        "Finally compiling and fitting the model. We used categorical crossentropy as the loss function and accuracy as the performance metric.\n",
        "The model will run for around 100 epochs depending upon when the early stopping stops the training."
      ]
    },
    {
      "metadata": {
        "id": "NvHuU4s6HEte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, validation_split = 0.2, epochs = 500, callbacks = [checkpoint, stop, learning_rate_reduction])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "109zG44hRhOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading Weights**\n",
        "\n",
        "We will have some weights been saved in the directory after successfully training the model. The names of the weights has name according to the validation accuracy (for eg: weight with 90% validaion accuracy score will have a name - model_ANN-0.90.h5). \n",
        "We then manually load the weight with best validation accuracy and finally make the predictions on test data.\n",
        "\n",
        "In my case the best weight was with validation accuracy of 53.3, so i load the weights with name: model_ANN-0.53.h5"
      ]
    },
    {
      "metadata": {
        "id": "go08YZTWReWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('model_ANN-0.53.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GM_Cbn_ATbYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the model performance is not satisfactory, we will use some different approach"
      ]
    },
    {
      "metadata": {
        "id": "qMmT3TjvDDJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Approach-2 (Using CNNs)"
      ]
    },
    {
      "metadata": {
        "id": "XDTpg5pFUA9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing Required Libraries**"
      ]
    },
    {
      "metadata": {
        "id": "MVu9Qk_g05CT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras.utils import np_utils\n",
        "import os as os\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVkc-6upUG6n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reading FIles**\n",
        "\n",
        "Using Python's pickle library to read pickle files.\n",
        "\n",
        "*   Training image data is present in train_images\n",
        "*   Training Labels are present in train_labels\n",
        "*   Test image data is present in test_images\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D37LW-uY1pf_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_image.pkl', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "with open('train_label.pkl', 'rb') as f:\n",
        "    train_labels = pickle.load(f)\n",
        "    \n",
        "with open('test_image.pkl', 'rb') as f:\n",
        "    test_images = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6w6RV6VVmBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "10Y4YDfWVxwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use to_categorical funciton from keras library to transform labels into one-hot-encoded vectors. The function returns one hot encoded vector with number of labels = maximum value of label present in the passed train_labels. SInce the labels present in the train_labels are {0, 2, 3, 6}, we mapped the labels as follows:\n",
        "\n",
        "0 -> 0 2 -> 1 3 -> 2 6 -> 3\n",
        "\n",
        "Before final submission, we will transform the results back to original labels\n",
        "\n",
        "We then split the training set into training and validation set using sklearn's train_test_split function, Following a general trend we will put 80% data into training and 20% into validation set."
      ]
    },
    {
      "metadata": {
        "id": "XAl9o2QzZH0r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(set(train_labels))\n",
        "    \n",
        "for i in range(len(train_labels)):\n",
        "  if train_labels[i] == 2:\n",
        "    train_labels[i] = 1\n",
        "  elif train_labels[i] == 3:\n",
        "    train_labels[i] = 2\n",
        "  elif train_labels[i] == 6:\n",
        "    train_labels[i] = 3\n",
        "    \n",
        "print(set(train_labels))\n",
        "\n",
        "train_complete_images, train_complete_labels = train_images, train_labels\n",
        "\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size = 0.2, random_state = 20)\n",
        "print(np.array(train_images).shape, np.array(train_labels).shape)\n",
        "print(np.array(val_images).shape, np.array(val_labels).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_nfufI7WtY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the pixel values of all the images in training, validation and test set are stored as one vector, we will need to reshape the vector to form a image out of it and to be able to run a CNN model.\n",
        "\n",
        "Since number of pixel for a training set were 784, an obvious guess was to try to reshape it into (28x28), because 28x28=784. We then checked whether the reshaped image make some sense or not by plotting it using PIL library of python.\n",
        "\n",
        "We will then perform **Feature Scaling** by dividing the training, validation and test images by 255(which is the max value of a pixel). This helps in better fitting the model in less time. "
      ]
    },
    {
      "metadata": {
        "id": "xClLJjWJ2mX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = np.array(train_images).reshape((6400, 28, 28, 1)).astype('uint8')\n",
        "img = Image.fromarray(train_images[5000].reshape((28, 28)).astype('uint8'), 'L')\n",
        "plt.imshow(img)\n",
        "\n",
        "val_images = np.array(val_images).reshape((1600, 28, 28, 1)).astype('uint8')\n",
        "img = Image.fromarray(val_images[50].reshape((28, 28)).astype('uint8'), 'L')\n",
        "plt.imshow(img)\n",
        "\n",
        "test_images = np.array(test_images).reshape((2000, 28, 28, 1)).astype('uint8')\n",
        "img = Image.fromarray(test_images[1500].reshape((28, 28)).astype('uint8'), 'L')\n",
        "plt.imshow(img)\n",
        "\n",
        "train_images = train_images/255.\n",
        "val_images = val_images/255.\n",
        "test_images = test_images/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etOWxTk1XwWU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We saw that the plotted image makes some sense and hence our guess to reshape the pixel array was correct"
      ]
    },
    {
      "metadata": {
        "id": "4ezJPa1ZYA3p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transform the train and validation labels to one-hot-encoded vectors"
      ]
    },
    {
      "metadata": {
        "id": "zkHs8OpR2yrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = np_utils.to_categorical(np.array(train_labels))\n",
        "val_labels = np_utils.to_categorical(np.array(val_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mu1qfbHcaxJr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Building the model**\n",
        "\n",
        "Used Convolutional, MaxPooling , Dropout, Batch Normalization and Flatten Layers to build a CNN model.\n",
        "The model uses padding parameter to be 'same' in order to not reduce the dimension of the the image at each layer, using Max pooling layers helps in extracting the main features in the previous layers. \n",
        "Batch Normalization maintains the mean to be 0 and deviation to be 1 for each layer, which helps in faster learning. \n",
        "We used Dropouts in between in order to avoid overfitting\n",
        "Finally flattened all the activation units to form a vector of 1 dimension and passed it through dense layers to eventually output 4 values. The last layer uses softmax activation function to output probabilities of occurrence of 4 classes."
      ]
    },
    {
      "metadata": {
        "id": "YSPmyGVNf_Od",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(28, 28, 1)))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        " \n",
        "# second CONV => RELU => CONV => RELU => POOL layer set\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# first (and only) set of FC => RELU layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        " \n",
        "# softmax classifier\n",
        "model.add(Dense(4))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "69AAKjWuezgy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Some Tuning before finally fitting the model**\n",
        "\n",
        "To save our model weights, we used ModelCheckpoint, it's parameters are set in order to save the weights to the current working directory every time validation accuracy improves.\n",
        "\n",
        "We used Early Stopping to stop training the model once the validation accuracy stops increasing after a set number of epochs (patience value).\n",
        "\n",
        "We used ReduceLROnPlateau to reduce the learning rate by a set factor each time validation accuracy stops increasing after a set number of epochs (patience value)."
      ]
    },
    {
      "metadata": {
        "id": "FmLo3l3ataf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"model_CNN-{val_acc:.2f}.h5\", monitor=\"val_acc\", verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=True, mode=\"max\", period=1)\n",
        "\n",
        "stop = EarlyStopping(monitor=\"val_acc\", patience=50, mode=\"max\")\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
        "                                            patience = 3,\n",
        "                                            #patience=5, \n",
        "                                            verbose=1, \n",
        "                                            factor = 0.5,\n",
        "                                            #factor=0.25, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TB_3IP4Ae5RO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Training The model**\n",
        "\n",
        "Finally compiling and fitting the model. We used categorical crossentropy as the loss function and accuracy as the performance metric.\n",
        "The model will run for around 120-130 epochs depending upon when the early stopping stops the training."
      ]
    },
    {
      "metadata": {
        "id": "d0-SY0TWgawC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_images, train_labels, validation_data = (val_images, val_labels), epochs=500, batch_size = 256, callbacks = [checkpoint, stop, learning_rate_reduction])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kFW8imFgFDR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading Weights**\n",
        "\n",
        "We will have some weights been saved in the directory after successfully training the model. The names of the weights has name according to the validation accuracy (for eg: weight with 90% validaion accuracy score will have a name - model_CNN-0.90.h5). \n",
        "We then manually load the weight with best validation accuracy and finally make the predictions on test data.\n",
        "\n",
        "In my case the best weight was with validation accuracy of 88.375, so i load the weights with name: model_CNN-0.89.h5"
      ]
    },
    {
      "metadata": {
        "id": "IbtoaE6HpOHq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('model_CNN-0.89.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nAOyTj0Dgorl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Making Predictions**\n",
        "\n",
        "Here we predicted the probabilities for test images and then used argmax function of numpy library to find the index of maximum probability among the 4 predicted probabilities. The result of this is the model's prediction of which class a test image belongs.\n",
        "\n",
        "We then transformed back the labels changed previously back to the original ones, using the reverse mapping of the previously discussed mapping"
      ]
    },
    {
      "metadata": {
        "id": "cKLUaTfpjGLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_images, verbose = 1)\n",
        "\n",
        "predictions=np.argmax(pred,axis=1)\n",
        "print(predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p-cMrnOMWezf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions_transformed = []\n",
        "for i in range(len(predictions)):\n",
        "    \n",
        "    if predictions[i] == 0:\n",
        "        predictions_transformed.append(0)\n",
        "        \n",
        "    if predictions[i] == 1:\n",
        "        predictions_transformed.append(2)\n",
        "        \n",
        "    elif predictions[i] == 2:\n",
        "        predictions_transformed.append(3)\n",
        "        \n",
        "    elif predictions[i] == 3:\n",
        "        predictions_transformed.append(6)\n",
        "        \n",
        "print(predictions_transformed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gkMgeCWry98B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.sum(np.array(predictions_transformed) == 0))\n",
        "print(np.sum(np.array(predictions_transformed) == 2))\n",
        "print(np.sum(np.array(predictions_transformed) == 3))\n",
        "print(np.sum(np.array(predictions_transformed) == 6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRxz8kL6ubvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Making The prediction File**\n",
        "\n",
        "After going through all 3 approaches, we can conclude that this approach works out to be the best, hence making the submission file according to this model's prediction"
      ]
    },
    {
      "metadata": {
        "id": "7uH31HJjupD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sub = []\n",
        "for i in range(len(predictions_transformed)):\n",
        "  temp = []\n",
        "  temp.append(i)\n",
        "  temp.append(predictions_transformed[i])\n",
        "  sub.append(temp)\n",
        "  \n",
        "df = pd.DataFrame(sub, columns =['image_index', 'class'])\n",
        "df.to_csv('Dhruv Agarwal.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFc8vlw1b-uC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Approach - 3(Using Pre-Trained CNNs)\n",
        "\n",
        "In this section I have used transfer learning on imagenet weights to classify test images"
      ]
    },
    {
      "metadata": {
        "id": "Pamo0I6TdoSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from keras.utils import np_utils\n",
        "import os as os\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sEOFP-_oqaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We do the similar data preprocessing as done in the second approach"
      ]
    },
    {
      "metadata": {
        "id": "Wwp6ViBXcBOC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_image.pkl', 'rb') as f:\n",
        "    train_images = pickle.load(f)\n",
        "\n",
        "with open('train_label.pkl', 'rb') as f:\n",
        "    train_labels = pickle.load(f)\n",
        "    \n",
        "for i in range(len(train_labels)):\n",
        "  if train_labels[i] == 2:\n",
        "    train_labels[i] = 1\n",
        "  elif train_labels[i] == 3:\n",
        "    train_labels[i] = 2\n",
        "  elif train_labels[i] == 6:\n",
        "    train_labels[i] = 3\n",
        "    \n",
        "print(set(train_labels))\n",
        "\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size = 0.2, random_state = 42)\n",
        "print(np.array(train_images).shape, np.array(train_labels).shape)\n",
        "print(np.array(val_images).shape, np.array(val_labels).shape)\n",
        "\n",
        "with open('test_image.pkl', 'rb') as f:\n",
        "    test_images = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "686mWoz7ozAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Gray scale equivalent of RGB for Pre-Trained CNN model**\n",
        "\n",
        "Since any pretrained model on imagenet weights expects 3 color channels (RGB) instead of 1 (in case of Grayscale), I used a trick to make the grayscale image compatible with imagenet models, The trick is to copy the exact same image(28x28) into 3 channels, i.e. convert it to a size of 28x28x3 where each channel of an image has the same 28x28 pixels.\n"
      ]
    },
    {
      "metadata": {
        "id": "qauymYMIcSa-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = np.dstack([train_images] * 3)\n",
        "test_images = np.dstack([test_images]*3)\n",
        "val_images = np.dstack([val_images]*3)\n",
        "                        \n",
        "print(train_images.shape, test_images.shape, val_images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJidqvKjctX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape(-1, 28,28,3)\n",
        "test_images= test_images.reshape (-1,28,28,3)\n",
        "val_images= val_images.reshape (-1,28,28,3)\n",
        "\n",
        "print(train_images.shape, test_images.shape, val_images.shape)\n",
        "print(np.array(train_labels).shape, np.array(val_labels).shape)\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "val_labels = np_utils.to_categorical(val_labels)\n",
        "print(np.array(train_labels).shape, np.array(val_labels).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aoGgXlQWqm3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**preprocess_input : Keras**\n",
        "\n",
        "We used preprocessing tool of keras, which performs preprocessing like transforming data to have mean of 0 and deviation of 1. This preprocessing is helpful in training the model faster.\n",
        "\n",
        "Since the VGG19 model which we are using expects a size of atlease (32, 32, 3), we need to resize our image from a size of (28, 28, 3) to a bigger size.\n",
        "I tries using various different sizes and found that (96, 96, 3) works the best, so I have used that only in this notebook."
      ]
    },
    {
      "metadata": {
        "id": "ALsXrX9vdl68",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = preprocess_input(train_images)\n",
        "test_images = preprocess_input(test_images)\n",
        "val_images = preprocess_input(val_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJtKcNFdfZ8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "train_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((96,96))) for im in train_images])\n",
        "test_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((96,96))) for im in test_images])\n",
        "val_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((96,96))) for im in val_images])\n",
        "\n",
        "print(train_images.shape, test_images.shape, val_images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_jVVkrjrkB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below we build a model which passes an image through VGG 19 model with top few layers removed and weights initialized with imagenet weights. The output of model is then passed through few layers to eventually give 4 outputs(probabilities of 4 classes, using softmax activation)"
      ]
    },
    {
      "metadata": {
        "id": "qvpV5T7peIdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1 = VGG19(include_top = False, input_shape = (96, 96, 3), weights = 'imagenet')\n",
        "x = model1.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "model = Model(inputs = model1.input, outputs = predictions)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PU7pgwzigvGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"model_CNN_Pretrained-{val_loss:.2f}.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=True, mode=\"min\", period=1)\n",
        "\n",
        "stop = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3KQJp-vOhUUt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gi2HgWnzhZWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, validation_data = (val_images, val_labels),\n",
        "                    epochs=100, callbacks = [checkpoint, stop], batch_size = 32\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlZIXPm9uIWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that the validation and train accuracies are quite unsatisfactory, hence we will go with approach-2 only"
      ]
    }
  ]
}